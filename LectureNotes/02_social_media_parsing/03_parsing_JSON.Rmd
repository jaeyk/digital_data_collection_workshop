---
title: "Parsing JSON"
author: "Jae Yeon Kim"
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Setup

```{r}
# Install packages
if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse, # tidyverse pkgs including purrr
  furrr, # parallel processing
  tictoc, # performance test
  tcltk, # GUI for choosing a dir path
  tidyjson
) # tidying JSON files

## Install the current development version from GitHub
devtools::install_github("jaeyk/tidytweetjson",
  dependencies = TRUE
)

library(tidytweetjson)
```

# Objectives

-   Learning chunk and pull strategy
-   Learning how `tidyjson` works
-   Learning how to apply `tidyjson` to tweets

# Chunk and Pull

## Problem

-   What if the size of the Twitter data you downloaded is too big (e.g., \>10GB) to do complex wrangling in R?

## Solution

![](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/chunk_pull.png)

Step1: Split the large JSON file in small chunks.

```{bash eval = FALSE}
#Divide the JSON file by 100 lines (tweets)

# Linux and Windows (in Bash)
$ split -100 search.jsonl

# macOS
$ gsplit -100 search.jsonl
```

-   After that, you will see several files appeared in the directory. Each of these files should have 100 tweets or fewer. All of these file names **should start with "x", as in "xaa".**

Step 2: Apply the parsing function to each chunk and pull all of these chunks together.

```{r eval = FALSE}

# You need to choose a Tweet JSON file
filepath <- file.choose()

# Assign the parsed result to the `df` object
# 11.28 sec elapsed to parse 17,928 tweets
tic()
df <- jsonl_to_df(filepath)
toc()
```

```{r eval = FALSE}
# Setup
n_cores <- availableCores() - 1

n_cores # This number depends on your computer spec.

plan(multiprocess, # multicore, if supported, otherwise multisession
  workers = n_cores
) # the maximum number of workers

# You need to designate a directory path where you saved the list of JSON files.

# 9.385 sec elapsed to parse 17,928 tweets

dirpath <- tcltk::tk_choose.dir()

tic()
df_all <- tidytweetjson::jsonl_to_df_all(dirpath)
toc()
```

# tidyjson

The [`tidyjson`](https://cran.r-project.org/web/packages/tidyjson/vignettes/introduction-to-tidyjson.html) package helps to use tidyverse framework to JSON data.

-   toy example

```{r}

# JSON collection; nested structure + keys and values
worldbank[1]

# Check out keys (objects)
worldbank %>%
  as.tbl_json() %>%
  gather_object() %>%
  filter(document.id == 1)
```

```{r}

# Get the values associated with the keys
worldbank %>%
  as.tbl_json() %>% # Turn JSON into tbl_json object
  enter_object("project_name") %>% # Enter the objects
  append_values_string() %>% # Append the values
  as_tibble() # To reduce the size of the file
```

# Application

-   The following example draws on my [tidytweetjson](https://github.com/jaeyk/tidytweetjson) R package. The package applies `tidyjson` to Tweets.

## Individual file

```{r}

jsonl_to_df <- function(file_path) {

  # Save file name

  file_name <- strsplit(
    x = file_path,
    split = "[/]"
  )

  file_name <- file_name[[1]][length(file_name[[1]])]

  # Import a Tweet JSON file

  listed <- read_json(file_path, format = c("jsonl"))

  # IDs of the tweets with country codes

  ccodes <- listed %>%
    enter_object("place") %>%
    enter_object("country_code") %>%
    append_values_string() %>%
    as_tibble() %>%
    rename("country_code" = "string")

  # IDs of the tweets with location

  locations <- listed %>%
    enter_object("user") %>%
    enter_object("location") %>%
    append_values_string() %>%
    as_tibble() %>%
    rename(location = "string")

  # Extract other key elements from the JSON file

  df <- listed %>%
    spread_values(
      id = jnumber("id"),
      created_at = jstring("created_at"),
      full_text = jstring("full_text"),
      retweet_count = jnumber("retweet_count"),
      favorite_count = jnumber("favorite_count"),
      user.followers_count = jnumber("user.followers_count"),
      user.friends_count = jnumber("user.friends_count")
    ) %>%
    as_tibble()

  message(paste("Parsing", file_name, "done."))

  # Full join
  outcome <- full_join(ccodes, df) %>% full_join(locations)

  # Or you can write this way: outcome <- reduce(list(df, ccodes, locations), full_join)

  # Select
  outcome %>% select(-c("document.id"))
}
```

## Many files

-   Set up parallel processing.

```{r}

n_cores <- availableCores() - 1

n_cores # This number depends on your computer spec.

plan(multiprocess, # multicore, if supported, otherwise multisession
  workers = n_cores
) # the maximum number of workers

```

-   Parsing in parallel.

**Review**

There are at least three ways you can use function + `purrr::map()`.

```{r eval = FALSE}

squared <- function(x) {
  x * 2
}

# Named function
map(1:3, squared)

# Anonymous function
map(1:3, function(x) {
  x * 2
})

# Using formula; ~ = formula, .x = input
map(1:3, ~ .x * 2)
```

```{r, eval = FALSE}

# Create a list of file paths
filename <- list.files(dir_path,
  pattern = "^x",
  full.names = TRUE
)

df <- filename %>%

  # Apply jsonl_to_df function to items on the list
  future_map(~ jsonl_to_df(.)) %>%

  # Full join the list of dataframes
  reduce(full_join,
    by = c(
      "id",
      "location",
      "country_code",
      "created_at",
      "full_text",
      "retweet_count",
      "favorite_count",
      "user.followers_count",
      "user.friends_count"
    )
  )

# Output
df
```
