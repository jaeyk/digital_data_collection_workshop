---
title: "Hydrating"
author: "Jae Yeon Kim"
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Objectives 

- Learning how hydrating works 
- Learning how to use [Twarc](https://github.com/DocNow/twarc) to communicate with Twitter's API

**Review question**

What are the main two types of Twitter's API?

# Hydrating: An Alternative Way to Collect Historical Twitter Data

- You can collect Twitter data using Twitter's API or you can hydrate Tweet IDs collected by other researchers. This is a good resource to collect historical Twitter data.

- [Covid-19 Twitter chatter dataset for scientic use](http://www.panacealab.org/covid19/) by Panacealab 
- [Women's March Dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5ZVMOR) by Littman and Park

- Harvard Dataverse has a number of dehydrated Tweet IDs that could be of interest to social scientists. 

![Dehydrated Tweet IDs](https://github.com/jaeyk/digital_data_collection_workshop/raw/master/misc/dehydrated_tweets.png)

# Twarc: one solution to (almost) all Twitter's API problems 

- Why Twarc?
  
  - A command-line tool and Python library that works for almost every Twitter's API related problem. 
  - It's really well-documented, tested, and maintained. 
     - [Twarc documentation](https://scholarslab.github.io/learn-twarc/06-twarc-command-basics) covers basic commands. 
     - [Tward-cloud documentation](https://twarc-cloud.readthedocs.io/_/downloads/en/stable/pdf/) explains how to collect data from Twitter's API using Twarc running in [Amazon Web Services](https://aws.amazon.com/) (AWS).
  - Twarc was developed as part of the [Documenting the Now](https://www.docnow.io/) project which was funded by the Mellon Foundation.
  
![One ring that rules them all.](https://vignette.wikia.nocookie.net/lotr/images/8/8b/DOiAi2WUEAE3A1Y.0.jpg/revision/latest/scale-to-width-down/699?cb=20200305221819)
  
- There's no reason to be afraid of using a command-line tool and Python library, even though you primarily use R. It's easy to embed [Python code](https://bookdown.org/yihui/rmarkdown/language-engines.html#python) and [shell scripts](https://bookdown.org/yihui/rmarkdown/language-engines.html#shell-scripts) in R Markdown.

- Even though you don't know how to write Python code or shell scripts, it's really useful to know how to integrate them in your R workflow.

- I assume that you have already installed [Python 3](https://www.python.org/download/releases/3.0/).

```{bash eval = FALSE}
pip3 install twarc
```

## Applications 

The following examples are created by [the University of Virginia library](http://digitalcollecting.lib.virginia.edu/toolkit/docs/social-media/twarc-commands/).

### Search 

- Download pre-existing tweets (7-day window) matching certain conditions

- In command-line, `>` = Create a file 

- I recommend running the following commands in the terminal because it's more stable than doing so in R Markdown. 

![You can type commands in the Terminal in R Studio.](https://github.com/jaeyk/digital_data_collection_workshop/raw/master/misc/terminal.png)

```{bash eval=FALSE}
# Key word 
twarc search blacklivesmatter > blm_tweets.jsonl
```

```{bash eval=FALSE}
# Hashtag 
twarc search '#blacklivesmatter' > blm_tweets_hash.jsonl
```

```{bash eval=FALSE}
# Hashtag + Language 
twarc search '#blacklivesmatter' --lang en > blm_tweets_hash.jsonl
```

- It is really important to **save these tweets into a `jsonl` format;** `jsonl` extension refers to JSON **Lines** files. This structure is useful for splitting JSON data into smaller chunks, if it is too large.

### Filter 

- Download tweets meeting certain conditions as they happen. 

```{bash eval=FALSE}
# Key word
twarc filter blacklivesmatter > blm_tweets.jsonl
```

### Sample 

- Use Twitter's random sample of recent tweets. 

```{bash eval=FALSE}
twarc sample > tweets.jsonl 
```

### Hydrate 

- Tweet IDs -> Tweets 

```{bash eval=FALSE}
twarc hydrate tweet_ids.txt > tweets.jsonl 
```

### Dehydrate 

- Hydrate <> Dehydrate 
- Tweets -> Tweet IDs

```{bash eval=FALSE}
twarc dehydrate tweets.jsonl > tweet_ids.txt
```

**Challenge**

1. Collect tweets contain some key words of your choice using `twarc search` and save them as `tweets.jsonl`. 

2. Using `less` command in the terminal, inspect `twarc.log`. 

3. Using `less` command in the terminal, inspect `tweets.json`. 
