---
title: "Automating digital data collection"
author: "Jae Yeon Kim"
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Objectives 

1. Getting the big picture: turning semi-structured data (input) into structured data (output) 

2. Learning how to automate data collecting and wrangling process

# Semi-structured data (input)

> Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. - [Wikipedia](https://en.wikipedia.org/wiki/Semi-structured_data#:~:text=Semi%2Dstructured%20data%20is%20a,and%20fields%20within%20the%20data.)

- Examples: `HTML (Hypertext Markup Language)` files (e.g., websites), `JSON (JavaScript Object Notation)` files (e.g., tweets), some `PDF (Portable Document Format)` files 

- Why should we care semi-structured data? 
    - Because this is what the data frontier looks like: # of unstructured data > # of semi-structured data > # of structured data 
    - There are easy and fast ways to turn semi-structured data into structured data (ideally in a tidy format) using R, Python, and command-line tools. See my own examples ([tidyethnicnews](https://github.com/jaeyk/tidyethnicnews) and [tidytweetjson](https://github.com/jaeyk/tidytweetjson)). 
    
![Figure 1. This is how the README file of this Git repository looks in raw HTML. Semi-structured data are everywhere.](https://github.com/jaeyk/digital_data_collection_workshop/raw/master/misc/html_example.png)

**Challenge 1** 
1. What's the data type (structured, semi-structured, and unstructured) of the document appeared in the left panel in Figure 1? 
2. What's the data type (structured, semi-structured, and unstructured) of the document appeared in the right panel in Figure 1? 


# Tidyverse dataframes (output)

> "Tidy data sets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table." - Hadley Wickham

1. Variables -> **Columns**
2. Observations -> **Rows**
3. Values -> **Cells**

![Figure 2. Tidy Data Example (Source: R for Data Science)](https://garrettgman.github.io/images/tidy-1.png)

If dataframes are tidy, it's easy to transform, visualize, model, and program them using tidyverse packages (a whole workflow).

![Figure 3. Tidyverse: an opinionated collection of R packages](https://miro.medium.com/max/960/0*mlPyX0NE0WQwEzpS.png)

- Nevertheless, don't be **religious**.

> In summary, tidy data is a useful conceptual idea and is often the right way to go for general, small data sets, but may not be appropriate for all problems. - Jeff Leek

For instance, in many data science applications, linear algebra-based computations are essential (e.g., [Principal Component Analysis](https://www.math.upenn.edu/~kazdan/312S13/JJ/PCA-JJ.pdf)). These computations are optimized to work on matrices, not tidy data frames (for more information, read [Jeff Leek's blog post](https://simplystatistics.org/2016/02/17/non-tidy-data/)).

# Workflow
  
## 1. Import/connect to a semi-structured file using `rvest,` `jsonlite,` `xml2,` `pdftools,` `tidyjson`, etc. 

- Google Scholar platform is notoriously difficult to scrape (for more information, please read [this *nature* column](https://www.nature.com/articles/d41586-018-04190-5) on scraping data from Google Scholar). 

- Any attempt to scrape content from Google's service may cause you to be suspended from using it. For more information, see the [Google Scholar robots.txt.](https://scholar.google.com/robots.txt) So, don't try to scrape it and be cautious with Google's service terms. 

> Don’t misuse our Services. For example, don’t interfere with our Services or try to access them using a method other than the interface and the instructions that we provide. - Google Terms of Service

- I wrote this tutorial only for the educational purpose.

```{r}
# Install packages 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(xml2, 
               rvest, 
               tidyverse, 
               testthat,
               tictoc,
               furrr)

# Citation information of Behind the Mule (1994) written by Michael Dawson 
document <- xml2::read_html("https://scholar.google.com/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=8941738289420139866&scipsc=")

document
```

## 2. Define target elements in the single file and extract them 

- [`readr`](https://readr.tidyverse.org/) package providers `parse_` functions that are useful for vector parsing. 
- [`stringr`](https://stringr.tidyverse.org/) package for string manipulations (e.g., using regular expressions in a tidy way). Quite useful for parsing PDF files (see [this example](https://themockup.blog/posts/2020-04-03-beer-and-pdftools-a-vignette/)).
- [`rvest`](https://github.com/tidyverse/rvest) package for parsing HTML (R equivalent to `beautiful soup` in Python)
- [`tidyjson`](https://github.com/sailthru/tidyjson) package for parsing JSON data

```{r}

document %>%
  rvest::html_nodes("div.gs_ab_mdw") # division class = gs_ab_mdw

doc <- document %>%
  rvest::html_nodes("div.gs_ab_mdw") %>%
  rvest::html_text()

readr::parse_number(doc[2])

```
## 3. Create a list of files (in this case URLs) to parse

```{r}

create_url <- function(i){

    pub_year <- 1994 
    
    base <- "https://scholar.google.com/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=8941738289420139866&scipsc="

    start_year <- paste0("as_ylo=", pub_year - 1 + i)
    
    end_year <- paste0("as_yhi=", pub_year + i)
    
    message(paste("citation information between", pub_year - 1 + i, pub_year + i))
    
    paste0(base, "&", start_year, "&", end_year)
  
}

url_list <- map(1:3, create_url)

# two arguments - map2() 
# 3 and more than 3 arguments - pmap()

```
- Why `purrr:map()` is awesome. 

![Figure 5. How map() works.](https://d33wubrfki0l68.cloudfront.net/f0494d020aa517ae7b1011cea4c4a9f21702df8b/2577b/diagrams/functionals/map.png)

    - Input: Takes a vector and a function 
    
    - Computation: Calls the function once for each element of the vector 
  
    - Output: Returns in a list or whatever data format you prefer
  
- Compare the following three examples. 

```{r}

df <- tibble("a" = -99,
             "b" = -99,
             "c" = -99,
             "d" = -99)
             
# Bad
df$a[df$a == -99] <- NA
df$b[df$b == -99] <- NA
df$c[df$c == -99] <- NA
df$d[df$d == -99] <- NA

# Slightly better
fix_missing <- function(x) {
  x[x == -99] <- NA
  x
}

df$a <- fix_missing(df$a)
df$b <- fix_missing(df$b)
df$c <- fix_missing(df$c)
df$d <- fix_missing(df$d)

# Good 
df <- map_df(df, fix_missing)

```

- Okay then how about for loop?

```{r}

# Built-in data 
data("airquality")

# 0.021 sec elapsed
tic()
out1 <- vector("double", ncol(airquality))

for (i in seq_along(airquality)){
  
  out1[[i]] <- mean(airquality[[i]], na.rm = TRUE)
  
}
toc()

# 0.006 sec elapsed 
tic()
out1 <- airquality %>% map_dbl(mean, na.rm = TRUE)
toc()

```
- In short, map() is more readable, faster, and easily extensive with other data science tasks using %>%. 

- Also, it's easy to include make your function run slowly (to reduce the burden on a server) using `purrr`. 

```{r}

# 0.008 sec elapsed 
tic()
url_list <- map(1:3, create_url)
toc()

# 0.01 sec elapsed 
tic()

slowly(
  url_list <- map(1:3, create_url), 
  rate_delay(pause = 2))

toc()

```
- If you want to make your function run faster, it's also easy to do that. For more information, please visit the [`furrr`](https://davisvaughan.github.io/furrr/) package website.

```{r}

# 6.172 sec elapsed 
tic()
mean100 <- map(1:1000000, mean)
toc()

# 5.047 sec elapsed 

plan(multisession, workers = 4)

tic()
mena100 <- future_map(1:1000000, mean)
toc()

```

## 4. Write a parsing function 

```{r}

find_hit <- function(url){
  
  # Read html data 
  doc <- read_html(curl::curl(url, handle = curl::new_handle("useragent" = "Mozilla/5.0"))) %>%
    html_nodes("div.gs_ab_mdw") %>%
    html_text()
  
  # Extract the text  
  text <- doc[2]
  
  # Parse the text 
  out <- readr::parse_number(text)
  
  # Unit testing 
  test_that("The class of the output should be numeric", {
  testthat::expect_equal(class(out), "numeric")
  })
  
  out
  
}

# testhat::expect_equal(class(2), "character")

```

> It’s not that we don’t test our code, it’s that we don’t store our tests so they can be re-run automatically. - Hadley Wickam 

- When you write a function, including an automatic test is a really useful skill to reduce your time for debugging. As Wickam's quote emphasized, we actually test our code when we make changes, we just don't systematically isolate and automate them. `testthat` packages helps you to accomplish this objective easily and quickly. Also, as Karl Broman recommended, ["when you find a bug, write a test."](https://kbroman.org/pkg_primer/pages/tests.html)

## 5. Automate parsing process

```{r}

slow_find_hit <- slowly(find_hit)
  
hits <- map_dbl(url_list, slow_find_hit)

hits

```
![Figure 4. This is one way you can fully automate the data collection process and visualize the outcome.](https://github.com/jaeyk/validating-two-linked-fates/raw/master/outputs/cite.png)
- Note that if you scrape too much information from the platform too quickly, you will be prohibited using it for a while.

## 6. If you need to parse a really large file ... 

- When a target file is too large (>10GB) and you need to do complex wrangling, you may want to take a divide-conquer-reduce strategy as outlined below. (Social media data stored in JSON are usually quite large.)

- Step 1 (divide): Split the large target file. (You can easily do this in command-line using `split` command. We're not learning this today ... )

- Step 2 (conquer): Wrangle each splitted file. (We already learned how to do this.)

- Step 3 (reduce): Combine them together. (Something new. This is what we're learning in this subsection of the workshop.)

- The key is using `purrr::reduce()` function.

![Figure 5. How reduce() works.](https://d33wubrfki0l68.cloudfront.net/9c239e1227c69b7a2c9c2df234c21f3e1c74dd57/eec0e/diagrams/functionals/reduce.png)

    - Input: Takes a vector of length n
  
    - Computation: Calls a function with a pair of values at a time
  
    - Output: Returns a vector of length 1
  
- The following toy example comes from Wickam's [Advanced R](https://adv-r.hadley.nz/functionals.html#reduce).

```{r}
# A list of the four lists of randomly selected 15 items from 1:10 using replacement 

toy_list <- map(1:4, ~ sample(x = 1:10, 
                              size = 15, 
                              replace = TRUE))

# Let's find common elements across these four lists 
out <- toy_list[[1]]

out <- intersect(out, toy_list[[2]])

out <- intersect(out, toy_list[[3]])

out <- intersect(out, toy_list[[4]])

out 

# Using reduce 
reduce(toy_list, intersect)

```
- The following real-world example comes from my [`tidytweetjson`](https://github.com/jaeyk/tidytweetjson/blob/master/R/jsonl_to_df_all.R) package.

```{r}
jsonl_to_df_all <- function(dir_path){

      # Create a list of the splitted JSON files
      
      filename <- list.files(dir_path, # Using list.files() is also a pretty neat technique for parsing PDF files stored in a particular directory.
                pattern = '^x',
                full.names = TRUE)
      
      df <- filename %>%
      
        # Apply jsonl_to_df function to items on the list
      
        future_map(~jsonl_to_df(.)) %>%
      
        # Full join the list of dataframes
      
        reduce(full_join,
             by = c("id",
                    "location",
                    "country_code",
                    "created_at",
                    "full_text",
                    "retweet_count",
                    "favorite_count",
                    "user.followers_count",
                    "user.friends_count"))
# Output
df
}

```
